# PathEval: Directed Input Generation Evaluation Set For LLM

This is an evaluation set for the directed/targeted input solving problem.
The goal of directed test input (a.k.a. targeted test input) generation is to automatically generate test inputs with a given target. 

Targeted inputs are widely used in software engineering
tasks such as bug reproducation (target is the bug location),
test suite argument (target is the code to cover) and combing
with other testing tools to get better overall performance.

We use it to benchmark the ability of large language models for generation input to reach a certain code location or particular result.

## Installation

### Linux (Debain)
Install gcc, g++, python3 and openjdk by the following commands.
```shell
apt update
apt install -y -q build-essential gcc g++ python3 python3-pip openjdk-11-jdk-headless libssl-dev
python3 -m pip install tqdm

git clone https://github.com/CGCL-codes/PathEval.git
cd PathEval
```

### Docker
```shell
git clone https://github.com/CGCL-codes/PathEval.git
cd PathEval

docker build -t patheval .
docker run -it --rm patheval /bin/bash

# cd /work in container
```

## Usage
**The evaluation will compile and execute untrusted model-generated data and code (see scripts/*_check.py). It is strongly encouraged to run this project in a sandbox (e.g., docker container).**


Users can simply use this dataset through a couple of APIs.

**Example**
```python
from patheval import set_dataset, read_problems, evaluate_one
# select the dataset that corresponds to the language
set_dataset("patheval_cpp") # or "patheval_java" or "patheval_py" or "logic_bombs_c"
# load the problems
problems = read_problems()
# give the completion from your LLMs and the evaluate result will return.

# completion = query(...)
evaluate_one(problems[0], completion)['pass'] # True / False
```

**APIs**

**`set_dataset(dataset_name)`**
- Purpose: Selects the dataset for evaluation.
- Parameters:
  - `dataset_name` (str): The name of the dataset to use. Valid values are:
    - `"patheval_cpp"`: C++ dataset
    - `"patheval_java"`: Java dataset
    - `"patheval_py"`: Python dataset
    - `"logic_bombs_c"`: logic bombs dataset (in C language)
- Returns: None

**`read_problems()`**
- Purpose: Loads the problems from the selected dataset.
- Parameters: None
- Returns: `problems` (list): A list of problem.

**`evaluate_one(problem, completion)`**
- Purpose: Evaluates the given completion for a single problem.
- Parameters:
  - `problem` (object): One problem from the `read_problems()` returned list.
  - `completion` (str): The completion generated by LLMs.
- Returns: the input `problem` dictionary, with an additional key:
  - `"pass"` (bool): Indicates whether the completion passes the problem

### Sample
For each sample, the following information is provided:
| Key | Description |
| ---- | -----------| 
| index | index in **this dataset** |
| humaneval_task_id | task id in **HumanEval** | 
| focal_method_name | function name of focal method |
| focal_method_para | parameters of focal method |
| focal_method_return_type | return value type of focal method |
| focal_method | code of focal method |
| target | code of target | 

*In logic_bombs samples, humaneval_task_id is replaced by logic_bombs_task_id.*

The samples are placed under `data` folder in `.jsonl` files, the samples in the three different files are semantically the same, but belong to different programming languages. 

An example C++ sample is shown as follw.
```json
{
  "index": 180,
  "humaneval_task_id": "CPP/53",
  "focal_method_name": "add",
  "focal_method_para": "(int x,int y)",
  "focal_method_return_type": "int",
  "focal_method": "#include<stdio.h>\n#include<stdlib.h>\nusing namespace std;\n#include<algorithm>\n#include<math.h>\nint add(int x,int y){\n    return x+y;\n}",
  "target": "#undef NDEBUG\n#include<assert.h>\n\nint main(){\n\tauto result = add(<FILL_ME>);\n\tassert(result==5);\n}"
}
```
We use `<FILL_ME>` to mark the position for LLMs to fill in.

## Known Issues
There is a very small difference in the number of samples in the dataset for the three programming languages due to the fact that this dataset was converted from [HumanEval-X](https://huggingface.co/datasets/THUDM/humaneval-x) using our automated methodology, where a very small number of samples failed in this process and were discarded.

## Citation
This is originally created for our paper "Towards Understanding the Effectiveness of Large Language Models on Directed Test Input Generation" (ASE 2024, to appear). The preview version will be uploaded soon.